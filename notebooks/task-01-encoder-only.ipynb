{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6066255,"sourceType":"datasetVersion","datasetId":3471819},{"sourceId":13695960,"sourceType":"datasetVersion","datasetId":8595896}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setting up enviorment","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate evaluate scikit-learn torch\n!pip install -U transformers huggingface_hub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:54:43.628578Z","iopub.execute_input":"2025-11-13T05:54:43.628842Z","iopub.status.idle":"2025-11-13T05:56:25.566150Z","shell.execute_reply.started":"2025-11-13T05:54:43.628820Z","shell.execute_reply":"2025-11-13T05:56:25.565406Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.36.0)\nCollecting huggingface_hub\n  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\nSuccessfully installed tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer,AutoModelForSequenceClassification,TrainingArguments,Trainer\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:56:25.567471Z","iopub.execute_input":"2025-11-13T05:56:25.567710Z","iopub.status.idle":"2025-11-13T05:57:01.736305Z","shell.execute_reply.started":"2025-11-13T05:56:25.567688Z","shell.execute_reply":"2025-11-13T05:57:01.735534Z"}},"outputs":[{"name":"stderr","text":"2025-11-13 05:56:42.106187: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763013402.524935      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763013402.628077      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, random, numpy as np, pandas as pd\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nimport inspect\nfrom transformers import TrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:01.745207Z","iopub.execute_input":"2025-11-13T05:57:01.745478Z","iopub.status.idle":"2025-11-13T05:57:01.873176Z","shell.execute_reply.started":"2025-11-13T05:57:01.745456Z","shell.execute_reply":"2025-11-13T05:57:01.872279Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Import file and analyze it ","metadata":{}},{"cell_type":"code","source":"Seed=42\nrandom.seed(Seed)\nnp.random.seed(Seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:01.873944Z","iopub.execute_input":"2025-11-13T05:57:01.874441Z","iopub.status.idle":"2025-11-13T05:57:01.888674Z","shell.execute_reply.started":"2025-11-13T05:57:01.874421Z","shell.execute_reply":"2025-11-13T05:57:01.887965Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sensitive-analysis/sentiment-analysis.csv', encoding='utf-8-sig')\nprint(\"Columns:\", df.columns.tolist())\nprint(df.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:01.889485Z","iopub.execute_input":"2025-11-13T05:57:01.889690Z","iopub.status.idle":"2025-11-13T05:57:01.957412Z","shell.execute_reply.started":"2025-11-13T05:57:01.889674Z","shell.execute_reply":"2025-11-13T05:57:01.956684Z"}},"outputs":[{"name":"stdout","text":"Columns: ['Text', ' Sentiment', ' Source', ' Date/Time', ' User ID', ' Location', ' Confidence Score']\n                                               Text  Sentiment         Source  \\\n0                              I love this product!   Positive        Twitter   \n1                         The service was terrible.   Negative   Yelp Reviews   \n2                            This movie is amazing!   Positive           IMDb   \n3  I'm so disappointed with their customer support.   Negative   Online Forum   \n4                Just had the best meal of my life!   Positive    TripAdvisor   \n\n              Date/Time       User ID      Location   Confidence Score  \n0   2023-06-15 09:23:14      @user123      New York               0.85  \n1   2023-06-15 11:45:32       user456   Los Angeles               0.65  \n2   2023-06-15 14:10:22   moviefan789        London               0.92  \n3   2023-06-15 17:35:11    forumuser1       Toronto               0.78  \n4   2023-06-16 08:50:59      foodie22         Paris               0.88  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\ndf[label_col] = df[label_col].str.strip().str.lower()\n\n# Create label mappings\nunique = sorted(df[label_col].unique())\nlabel2id = {lab: i for i, lab in enumerate(unique)}\nid2label = {i: lab for lab, i in label2id.items()}\n\n# Add numeric label column\ndf[\"label\"] = df[label_col].map(label2id)\n\n# Show mapping and sample rows\nprint(\"Label map:\", label2id)\nprint(df[[text_col, label_col, \"label\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:41.414155Z","iopub.execute_input":"2025-11-13T05:57:41.414438Z","iopub.status.idle":"2025-11-13T05:57:41.424158Z","shell.execute_reply.started":"2025-11-13T05:57:41.414416Z","shell.execute_reply":"2025-11-13T05:57:41.423238Z"}},"outputs":[{"name":"stdout","text":"Label map: {'negative': 0, 'positive': 1}\n                                               Text Sentiment  label\n0                              I love this product!  positive      1\n1                         The service was terrible.  negative      0\n2                            This movie is amazing!  positive      1\n3  I'm so disappointed with their customer support.  negative      0\n4                Just had the best meal of my life!  positive      1\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Preprocessing\n### we may observe our sentiments are positive or negative convert them to digit 0 or 1","metadata":{}},{"cell_type":"markdown","source":"# Lets convert our data set into Hugging face format \n### for testing we are using small dataset","metadata":{}},{"cell_type":"code","source":"data=Dataset.from_pandas(df[[text_col,\"label\"]])\ndata_split=data.train_test_split(test_size=0.2,seed=Seed)\ndataset=DatasetDict({\"train\":data_split[\"train\"],\"validation\":data_split[\"test\"]})\nprint(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:41.425345Z","iopub.execute_input":"2025-11-13T05:57:41.425721Z","iopub.status.idle":"2025-11-13T05:57:41.457337Z","shell.execute_reply.started":"2025-11-13T05:57:41.425693Z","shell.execute_reply":"2025-11-13T05:57:41.456491Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Text', 'label'],\n        num_rows: 364\n    })\n    validation: Dataset({\n        features: ['Text', 'label'],\n        num_rows: 92\n    })\n})\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Importing our BERT Model","metadata":{}},{"cell_type":"code","source":"model_base = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_base)\n\ndef preprocess(batch):\n    return tokenizer(batch[text_col], padding=\"max_length\", max_length=128, truncation=True)\n\ndataset = dataset.map(preprocess, batched=True)\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:41.458652Z","iopub.execute_input":"2025-11-13T05:57:41.459064Z","iopub.status.idle":"2025-11-13T05:57:41.742740Z","shell.execute_reply.started":"2025-11-13T05:57:41.459021Z","shell.execute_reply":"2025-11-13T05:57:41.741852Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/364 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e34f79c0bd74e79aff6cf9dd3b6d326"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/92 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ba6ee530f64c76ab251fb91abc1d64"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"num_labels=len(label2id)\nmodel=AutoModelForSequenceClassification.from_pretrained(model_base,num_labels=num_labels,id2label=id2label,label2id=label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:41.743623Z","iopub.execute_input":"2025-11-13T05:57:41.744107Z","iopub.status.idle":"2025-11-13T05:57:41.873615Z","shell.execute_reply.started":"2025-11-13T05:57:41.744080Z","shell.execute_reply":"2025-11-13T05:57:41.872896Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# evaulation metrices\nfrom inspect import signature\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_macro\": f1_score(labels, preds, average=\"macro\"),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:57:41.875350Z","iopub.execute_input":"2025-11-13T05:57:41.875616Z","iopub.status.idle":"2025-11-13T05:57:41.879740Z","shell.execute_reply.started":"2025-11-13T05:57:41.875600Z","shell.execute_reply":"2025-11-13T05:57:41.879083Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# hugging face default parameters for model training \ntraining_args = TrainingArguments(\n    output_dir=\"./bert_sentiment\",\n    learning_rate=2e-5,\n    eval_strategy=\"epoch\", \n    save_strategy=\"epoch\",\n    \n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    report_to=\"none\",\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:58:21.174773Z","iopub.execute_input":"2025-11-13T05:58:21.175155Z","iopub.status.idle":"2025-11-13T05:58:21.241405Z","shell.execute_reply.started":"2025-11-13T05:58:21.175124Z","shell.execute_reply":"2025-11-13T05:58:21.240540Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_39/1875103790.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:58:21.242636Z","iopub.execute_input":"2025-11-13T05:58:21.242939Z","iopub.status.idle":"2025-11-13T05:58:22.000144Z","shell.execute_reply.started":"2025-11-13T05:58:21.242911Z","shell.execute_reply":"2025-11-13T05:58:21.999324Z"}},"outputs":[{"name":"stdout","text":"4.57.1\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:58:22.002394Z","iopub.execute_input":"2025-11-13T05:58:22.002889Z","iopub.status.idle":"2025-11-13T05:59:10.601344Z","shell.execute_reply.started":"2025-11-13T05:58:22.002866Z","shell.execute_reply":"2025-11-13T05:59:10.600698Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [115/115 00:47, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000400</td>\n      <td>0.000144</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000100</td>\n      <td>0.000066</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000100</td>\n      <td>0.000048</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000100</td>\n      <td>0.000041</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000100</td>\n      <td>0.000039</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=115, training_loss=0.00023278909755627747, metrics={'train_runtime': 47.9871, 'train_samples_per_second': 37.927, 'train_steps_per_second': 2.396, 'total_flos': 119715530188800.0, 'train_loss': 0.00023278909755627747, 'epoch': 5.0})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"pred_out = trainer.predict(dataset[\"validation\"])\npreds = np.argmax(pred_out.predictions, axis=-1)\nlabels = pred_out.label_ids\n\nprint(\"Accuracy:\", accuracy_score(labels, preds))\nprint(\"Macro F1:\", f1_score(labels, preds, average=\"macro\"))\nlabel_names = [id2label[i] for i in sorted(set(labels))] \nprint(\"\\nClassification Report:\\n\", classification_report(labels, preds, target_names=label_names))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(labels, preds))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:59:10.603285Z","iopub.execute_input":"2025-11-13T05:59:10.603608Z","iopub.status.idle":"2025-11-13T05:59:11.219103Z","shell.execute_reply.started":"2025-11-13T05:59:10.603588Z","shell.execute_reply":"2025-11-13T05:59:11.218342Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 1.0\nMacro F1: 1.0\n\nClassification Report:\n               precision    recall  f1-score   support\n\n    negative       1.00      1.00      1.00        39\n    positive       1.00      1.00      1.00        53\n\n    accuracy                           1.00        92\n   macro avg       1.00      1.00      1.00        92\nweighted avg       1.00      1.00      1.00        92\n\n\nConfusion Matrix:\n [[39  0]\n [ 0 53]]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"trainer.save_model(\"./bert_sentiment/final\")\ntokenizer.save_pretrained(\"./bert_sentiment/final\")\nprint(\"Saved to ./bert_sentiment/final\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:59:11.219951Z","iopub.execute_input":"2025-11-13T05:59:11.220250Z","iopub.status.idle":"2025-11-13T05:59:13.146738Z","shell.execute_reply.started":"2025-11-13T05:59:11.220224Z","shell.execute_reply":"2025-11-13T05:59:13.146105Z"}},"outputs":[{"name":"stdout","text":"Saved to ./bert_sentiment/final\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import torch\n\ntest_texts = [\n    \"I thought it would be terrible but it exceeded my expectations!\",\n    \"The product isn't bad but the customer service ruined everything.\",\n    \"Not the best, not the worst, just okay I guess.\",\n    \"I wanted to love it so badly but it let me down completely.\",\n    \"Despite the flaws, I'm surprisingly happy with my purchase.\",\n    \"The reviews said it was amazing but I found it utterly disappointing.\",\n    \"It's fine if you have low expectations, otherwise you'll hate it.\",\n    \"I can't believe how much I regret buying this piece of junk.\",\n    \"Mixed feelings - great price but questionable quality.\",\n    \"Honestly expected nothing and still managed to be impressed!\",\n    \"Would have been perfect if not for that one major issue.\",\n    \"I'm not sure if I like it or not, very confusing experience.\",\n    \"Started great but ended up being a waste of money.\",\n    \"The worst part? It actually works but I still hate using it.\",\n    \"Surprisingly decent for the price, can't complain much.\"\n]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntest_inputs = tokenizer(test_texts, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\")\ntest_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**test_inputs)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n\nresults = pd.DataFrame({\n    \"Text\": test_texts,\n    \"Predicted_Label\": [id2label[pred.item()] for pred in predictions],\n    \"Confidence\": torch.softmax(outputs.logits, dim=-1).max(dim=-1).values.cpu().numpy()\n})\n\nprint(results.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:59:13.147462Z","iopub.execute_input":"2025-11-13T05:59:13.147691Z","iopub.status.idle":"2025-11-13T05:59:13.460816Z","shell.execute_reply.started":"2025-11-13T05:59:13.147675Z","shell.execute_reply":"2025-11-13T05:59:13.460096Z"}},"outputs":[{"name":"stdout","text":"                                                                 Text Predicted_Label  Confidence\n      I thought it would be terrible but it exceeded my expectations!        negative    0.999765\n    The product isn't bad but the customer service ruined everything.        negative    0.999848\n                      Not the best, not the worst, just okay I guess.        positive    0.949247\n          I wanted to love it so badly but it let me down completely.        negative    0.993570\n          Despite the flaws, I'm surprisingly happy with my purchase.        positive    0.999775\nThe reviews said it was amazing but I found it utterly disappointing.        negative    0.999819\n    It's fine if you have low expectations, otherwise you'll hate it.        negative    0.999820\n         I can't believe how much I regret buying this piece of junk.        negative    0.999788\n               Mixed feelings - great price but questionable quality.        negative    0.999864\n         Honestly expected nothing and still managed to be impressed!        positive    0.999814\n             Would have been perfect if not for that one major issue.        positive    0.999799\n         I'm not sure if I like it or not, very confusing experience.        negative    0.999839\n                   Started great but ended up being a waste of money.        negative    0.999852\n         The worst part? It actually works but I still hate using it.        negative    0.999831\n              Surprisingly decent for the price, can't complain much.        positive    0.888582\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from google.colab import files\nimport shutil\nshutil.make_archive('bert_sentiment_model', 'zip', './bert_sentiment/final')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T05:59:13.461596Z","iopub.execute_input":"2025-11-13T05:59:13.461875Z","iopub.status.idle":"2025-11-13T05:59:35.306390Z","shell.execute_reply.started":"2025-11-13T05:59:13.461849Z","shell.execute_reply":"2025-11-13T05:59:35.305686Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/bert_sentiment_model.zip'"},"metadata":{}}],"execution_count":30}]}