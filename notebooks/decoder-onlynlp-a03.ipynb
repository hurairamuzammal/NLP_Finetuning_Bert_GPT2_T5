{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13579557,"sourceType":"datasetVersion","datasetId":8619531}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U transformers datasets peft accelerate sacrebleu evaluate tokenizers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:14:21.048333Z","iopub.execute_input":"2025-11-03T11:14:21.048626Z","iopub.status.idle":"2025-11-03T11:15:55.589306Z","shell.execute_reply.started":"2025-11-03T11:14:21.048602Z","shell.execute_reply":"2025-11-03T11:15:55.588547Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.1.1)\nCollecting datasets\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nCollecting peft\n  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\nCollecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\nCollecting tokenizers\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.8.3)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: pyarrow, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, datasets, accelerate, sacrebleu, peft, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.1.1\n    Uninstalling datasets-4.1.1:\n      Successfully uninstalled datasets-4.1.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.11.0 datasets-4.3.0 evaluate-0.4.6 huggingface-hub-0.36.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.17.1 portalocker-3.2.0 pyarrow-22.0.0 sacrebleu-2.5.1 tokenizers-0.22.1 transformers-4.57.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, json, random, glob, torch\nrandom.seed(42)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig, get_peft_model\nimport sacrebleu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:15:55.590661Z","iopub.execute_input":"2025-11-03T11:15:55.590869Z","iopub.status.idle":"2025-11-03T11:16:19.886598Z","shell.execute_reply.started":"2025-11-03T11:15:55.590850Z","shell.execute_reply":"2025-11-03T11:16:19.885855Z"}},"outputs":[{"name":"stderr","text":"2025-11-03 11:16:06.957807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762168567.150329      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762168567.203747      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json, glob, random\nrandom.seed(42)\ncandidates = glob.glob(\"/kaggle/input/**/sourceData.jsonl\", recursive=True) + glob.glob(\"sourceData.jsonl\")\nif not candidates: raise FileNotFoundError(\"upload sourceData.jsonl\")\nsrc = candidates[0]\nrecords = []\nwith open(src, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        if line.strip():\n            try:\n                o = json.loads(line)\n                if \"pseudo\" in o and \"code\" in o: records.append(o)\n            except: pass\nrandom.shuffle(records)\nn = len(records)\nsplit = min(2000, int(0.8 * n))\ntrain, val = records[:split], records[split:split+500]\ntrain_file = \"/kaggle/working/train.jsonl\"\neval_file = \"/kaggle/working/eval.jsonl\"\nwith open(train_file, \"w\", encoding=\"utf-8\") as f:\n    for r in train: f.write(json.dumps(r) + \"\\n\")\nwith open(eval_file, \"w\", encoding=\"utf-8\") as f:\n    for r in val: f.write(json.dumps(r) + \"\\n\")\nprint(\"train:\", len(train), \"eval:\", len(val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:16:19.887975Z","iopub.execute_input":"2025-11-03T11:16:19.888833Z","iopub.status.idle":"2025-11-03T11:16:20.183690Z","shell.execute_reply.started":"2025-11-03T11:16:19.888813Z","shell.execute_reply":"2025-11-03T11:16:20.183015Z"}},"outputs":[{"name":"stdout","text":"train: 2000 eval: 500\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nimport torch\n\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=False\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"c_attn\",\"c_fc\", \"c_proj\"],\n    lora_dropout=0.01,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:16:20.185080Z","iopub.execute_input":"2025-11-03T11:16:20.185703Z","iopub.status.idle":"2025-11-03T11:16:28.094759Z","shell.execute_reply.started":"2025-11-03T11:16:20.185677Z","shell.execute_reply":"2025-11-03T11:16:28.093954Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d56082f8ee646aca55b06257f8852d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed25783832241589f331575635750d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f82126bb39fb4c3397ddeeb0fd551c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1af5cb6cfe7a422bbf82171ed5b58b35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b25befcf5fa4afaa9da8e359d4a50be"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7776614915641b6a4d088643321cb89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2ee53dc65543038286ab538c77c6c1"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,359,296 || all params: 126,799,104 || trainable%: 1.8607\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\noutput_dir = \"/kaggle/working/gpt2-lora-improved\"\n\ndef tokenize_function(examples):\n    texts = [f\"<|pseudocode|>{p}<|code|>{c}<|end|>\" for p, c in zip(examples[\"pseudo\"], examples[\"code\"])]\n    return tokenizer(texts, truncation=True, max_length=384, padding=\"max_length\")\n\ndataset = load_dataset(\"json\", data_files={\"train\": train_file, \"eval\": eval_file})\ntokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nargs = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    fp16=True,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\",\n    logging_dir=\"/kaggle/working/logs\",\n    warmup_steps=50,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"eval\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntrainer.train()\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"GPT-2 model saved to {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:16:28.095417Z","iopub.execute_input":"2025-11-03T11:16:28.095663Z","iopub.status.idle":"2025-11-03T11:21:46.285290Z","shell.execute_reply.started":"2025-11-03T11:16:28.095646Z","shell.execute_reply":"2025-11-03T11:21:46.284676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9813bee79c64c58ad87f98b83066069"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating eval split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9a0fdbb1e8410f9023dfff2e019970"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7ce294637524e5283b8bebe38e16da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"946717b1d01e4d17abae3d3630b3e2e1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_37/2177806783.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [750/750 05:14, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.261000</td>\n      <td>1.086705</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.106000</td>\n      <td>0.973546</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.028600</td>\n      <td>0.934587</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"GPT-2 model saved to /kaggle/working/gpt2-lora-improved\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import sacrebleu, json, torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForCausalLM.from_pretrained(output_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Working on calculating BLEU\")\nhyps, refs = [], []\nwith open(eval_file, \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if i >= 50: break\n        ex = json.loads(line)\n        prompt = \"<|pseudocode|>\" + ex[\"pseudo\"].strip() + \"<|code|>\"\n        ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n        out = model.generate(ids, max_length=256, num_return_sequences=1, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n        pred = tokenizer.decode(out[0], skip_special_tokens=True)\n        gen = pred.split(\"<|code|>\",1)[1].strip() if \"<|code|>\" in pred else pred.strip()\n        hyps.append(gen)\n        refs.append([ex[\"code\"].strip()])\n\nbleu = sacrebleu.corpus_bleu(hyps, refs)\nprint(\"BLEU:\", bleu.score)\nwith open(\"/kaggle/working/predictions.jsonl\",\"w\") as f:\n    for r,h in zip(refs,hyps): f.write(json.dumps({\"reference\":r[0],\"prediction\":h})+\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:21:46.286022Z","iopub.execute_input":"2025-11-03T11:21:46.286206Z","iopub.status.idle":"2025-11-03T11:24:19.564189Z","shell.execute_reply.started":"2025-11-03T11:21:46.286191Z","shell.execute_reply":"2025-11-03T11:24:19.563513Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Working on calculating BLEU\nBLEU: 66.92643973076053\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Testing our model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\noutput_dir = \"/kaggle/working/gpt2-lora-improved\"\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir)\nmodel = AutoModelForCausalLM.from_pretrained(output_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef generate_code(pseudocode, max_length=256, temperature=0.7):\n    prompt = f\"<|pseudocode|>{pseudocode.strip()}<|code|>\"\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        temperature=temperature,\n        do_sample=True,\n        top_p=0.95,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    # Stop at the first <|end|> token\n    if \"<|end|>\" in generated:\n        generated = generated.split(\"<|end|>\")[0]\n\n    # Extract only code part\n    code = generated.split(\"<|code|>\", 1)[1].strip() if \"<|code|>\" in generated else generated.strip()\n\n    return code\n\n\n# ==== TEST EXAMPLES ====\nexamples = [\n    (\"Factorial Function\", \"\"\"\nfunction to calculate factorial of a number\ninput: n (integer)\nif n is 0 or 1, return 1\nelse multiply n by factorial of n-1\n\"\"\"),\n    (\"Find Maximum\", \"\"\"\nfunction to find maximum element in a list\ninput: arr (list of numbers)\ninitialize max_val to first element\nloop through each element in arr\nif element is greater than max_val, update max_val\nreturn max_val\n\"\"\"),\n    (\"Palindrome Check\", \"\"\"\nfunction to check if a string is palindrome\ninput: s (string)\nreverse the string and store in reversed_s\nif s equals reversed_s, return True\nelse return False\n\"\"\")\n]\n\nfor title, pseudo in examples:\n    print(\"=\" * 50)\n    print(f\"TEST EXAMPLE: {title}\")\n    print(\"=\" * 50)\n    print(\"PSEUDOCODE:\")\n    print(pseudo)\n    print(\"\\nGENERATED CODE:\")\n    print(generate_code(pseudo))\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:24:19.564920Z","iopub.execute_input":"2025-11-03T11:24:19.565165Z","iopub.status.idle":"2025-11-03T11:24:30.432462Z","shell.execute_reply.started":"2025-11-03T11:24:19.565147Z","shell.execute_reply":"2025-11-03T11:24:30.431662Z"}},"outputs":[{"name":"stdout","text":"==================================================\nTEST EXAMPLE: Factorial Function\n==================================================\nPSEUDOCODE:\n\nfunction to calculate factorial of a number\ninput: n (integer)\nif n is 0 or 1, return 1\nelse multiply n by factorial of n-1\n\n\nGENERATED CODE:\ndef main():\n    n = 0\n      = 0\n     n = input().split()\n      = factorial(n, 1)\n\n==================================================\nTEST EXAMPLE: Find Maximum\n==================================================\nPSEUDOCODE:\n\nfunction to find maximum element in a list\ninput: arr (list of numbers)\ninitialize max_val to first element\nloop through each element in arr\nif element is greater than max_val, update max_val\nreturn max_val\n\n\nGENERATED CODE:\ndef main():\n    = 0\n     = 0\n    arr(1, 1) = map(int, input().split())\n    if element == max_val:\n           return max_val\n\n==================================================\nTEST EXAMPLE: Palindrome Check\n==================================================\nPSEUDOCODE:\n\nfunction to check if a string is palindrome\ninput: s (string)\nreverse the string and store in reversed_s\nif s equals reversed_s, return True\nelse return False\n\n\nGENERATED CODE:\ndef main():\n    = 0\n    s = input().split()\n     if s == reversed_s:\n          return True\n        else:\n              return False\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import shutil\n\nmodel_dir_gpt2 = \"/kaggle/working/gpt2-lora-improved\"\nzip_path_gpt2 = \"/kaggle/working/gpt2-lora-improved\"\nshutil.make_archive(zip_path_gpt2, 'zip', model_dir_gpt2)\n\nprint(f\"GPT-2 model zipped: {zip_path_gpt2}.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:24:30.433362Z","iopub.execute_input":"2025-11-03T11:24:30.433889Z","iopub.status.idle":"2025-11-03T11:24:34.258845Z","shell.execute_reply.started":"2025-11-03T11:24:30.433859Z","shell.execute_reply":"2025-11-03T11:24:34.258148Z"}},"outputs":[{"name":"stdout","text":"GPT-2 model zipped: /kaggle/working/gpt2-lora-improved.zip\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Fine Tuning CodeParrrot","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model\nimport torch\n\nmodel_name = \"codeparrot/codeparrot-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=False\n)\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"c_attn\", \"c_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:24:34.259577Z","iopub.execute_input":"2025-11-03T11:24:34.259795Z","iopub.status.idle":"2025-11-03T11:24:44.130399Z","shell.execute_reply.started":"2025-11-03T11:24:34.259779Z","shell.execute_reply":"2025-11-03T11:24:44.129651Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/259 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2679051f9a0b41659d1b30092b7f38a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b5bcd690dc4862ae06d18d7bf31a89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93491998158c41929a1b273163219ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0687d06a057d45039e56f90dd68c62a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a2b852000a43f19a945acc6c92a5d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/903 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"887fe29037ed4d7b87da887e8c4d9b29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/457M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a112806ed2374b5abd4cec11a13719da"}},"metadata":{}},{"name":"stdout","text":"trainable params: 1,622,016 || all params: 112,630,272 || trainable%: 1.4401\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\noutput_dir_codeparrot = \"/kaggle/working/codeparrot-lora-improved\"\n\ndef tokenize_function(examples):\n    texts = [f\"<|pseudocode|>{p}<|code|>{c}<|end|>\" for p, c in zip(examples[\"pseudo\"], examples[\"code\"])]\n    return tokenizer(texts, truncation=True, max_length=384, padding=\"max_length\")\n\ndataset = load_dataset(\"json\", data_files={\"train\": train_file, \"eval\": eval_file})\ntokenized = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nargs = TrainingArguments(\n    output_dir=output_dir_codeparrot,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    fp16=True,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    logging_steps=50,\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\",\n    logging_dir=\"/kaggle/working/logs_codeparrot\",\n    warmup_steps=50,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    eval_dataset=tokenized[\"eval\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\nmodel.save_pretrained(output_dir_codeparrot)\ntokenizer.save_pretrained(output_dir_codeparrot)\nprint(f\"CodeParrot model saved to {output_dir_codeparrot}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:24:44.132498Z","iopub.execute_input":"2025-11-03T11:24:44.132720Z","iopub.status.idle":"2025-11-03T11:28:46.819039Z","shell.execute_reply.started":"2025-11-03T11:24:44.132703Z","shell.execute_reply":"2025-11-03T11:28:46.818337Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20dfe600e6bd4594a03ecd0bcc0ea364"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64ba91b5e4e4412ca559a7cb062305c8"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_37/477442040.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe model is already on multiple devices. Skipping the move to device specified in `args`.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 0, 'bos_token_id': 0, 'pad_token_id': 0}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [375/375 03:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.670700</td>\n      <td>1.493802</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/457M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44159f12842a4e7689f5aaefc5eb2fd7"}},"metadata":{}},{"name":"stdout","text":"CodeParrot model saved to /kaggle/working/codeparrot-lora-improved\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import json, time, torch, sacrebleu\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbase = \"codeparrot/codeparrot-small\"\nadapter_dir = \"/kaggle/working/codeparrot-lora-improved\"\neval_file = \"/kaggle/working/eval.jsonl\"\n\ntokenizer = AutoTokenizer.from_pretrained(base)\nbase_model = AutoModelForCausalLM.from_pretrained(base).to(device)\nmodel = PeftModel.from_pretrained(base_model, adapter_dir).to(device)\nmodel.eval()\n\nhyps, refs = [], []\nwith open(eval_file, \"r\", encoding=\"utf-8\") as f:\n    for i, line in enumerate(f):\n        if not line.strip(): continue\n        ex = json.loads(line)\n        prompt = f\"<|pseudocode|>{ex['pseudo'].strip()}<|code|>\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=384).to(device)\n\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens=128,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n\n        pred = tokenizer.decode(out[0], skip_special_tokens=False)\n        if \"<|end|>\" in pred: pred = pred.split(\"<|end|>\")[0]\n        gen = pred.split(\"<|code|>\", 1)[1].strip() if \"<|code|>\" in pred else pred.strip()\n\n        hyps.append(gen)\n        refs.append(ex.get(\"code\",\"\").strip())\n\n        if (i+1) % 10 == 0:\n            print(f\"[{i+1}] examples processed\", flush=True)\n        if i >= 99:\n            break\n\nbleu = sacrebleu.corpus_bleu(hyps, [refs])\nprint(\"BLEU:\", bleu.score)\n\nwith open(\"/kaggle/working/codeparrot_predictions.jsonl\", \"w\", encoding=\"utf-8\") as out:\n    for r, h in zip(refs, hyps):\n        out.write(json.dumps({\"reference\": r, \"prediction\": h}, ensure_ascii=False) + \"\\n\")\nprint(\"Predictions saved to /kaggle/working/codeparrot_predictions.jsonl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:28:46.819808Z","iopub.execute_input":"2025-11-03T11:28:46.820049Z","iopub.status.idle":"2025-11-03T11:31:51.376369Z","shell.execute_reply.started":"2025-11-03T11:28:46.820023Z","shell.execute_reply":"2025-11-03T11:31:51.375743Z"}},"outputs":[{"name":"stdout","text":"[10] examples processed\n[20] examples processed\n[30] examples processed\n[40] examples processed\n[50] examples processed\n[60] examples processed\n[70] examples processed\n[80] examples processed\n[90] examples processed\n[100] examples processed\nBLEU: 33.1791793675522\nPredictions saved to /kaggle/working/codeparrot_predictions.jsonl\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\noutput_dir_codeparrot = \"/kaggle/working/codeparrot-lora-improved\"\n\ntokenizer = AutoTokenizer.from_pretrained(output_dir_codeparrot)\nmodel = AutoModelForCausalLM.from_pretrained(output_dir_codeparrot).to(device)\n\ndef generate_code(pseudocode, max_length=384):\n    prompt = f\"<|pseudocode|>{pseudocode.strip()}<|code|>\"\n    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n\n    outputs = model.generate(\n        inputs,\n        max_length=max_length,\n        temperature=0.7,\n        top_p=0.95,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    gen = tokenizer.decode(outputs[0], skip_special_tokens=False)\n\n    # Stop at first <|end|> if exists\n    if \"<|end|>\" in gen:\n        gen = gen.split(\"<|end|>\")[0]\n\n    #Extract only the part after <|code|>\n    code = gen.split(\"<|code|>\", 1)[1].strip() if \"<|code|>\" in gen else gen.strip()\n    return code\n\n\n# ==== TEST CASES ====\ntests = {\n    \"Factorial\": \"\"\"function to calculate factorial of a number\ninput: n\nif n is 0 or 1 return 1\nelse return n * factorial(n-1)\"\"\",\n\n    \"Find Maximum\": \"\"\"function to find maximum in list\ninput: arr\ninitialize max to first element\nloop through arr\nif element > max update max\nreturn max\"\"\",\n\n    \"Palindrome Check\": \"\"\"function to check palindrome\ninput: s\nreverse the string\nif s equals reversed return True\nelse return False\"\"\"\n}\n\nfor name, pseudo in tests.items():\n    print(\"\\n\" + \"=\"*60)\n    print(f\"TEST: {name}\")\n    print(\"=\"*60)\n    print(generate_code(pseudo))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:31:51.377435Z","iopub.execute_input":"2025-11-03T11:31:51.377717Z","iopub.status.idle":"2025-11-03T11:32:07.873732Z","shell.execute_reply.started":"2025-11-03T11:31:51.377697Z","shell.execute_reply":"2025-11-03T11:32:07.872919Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nTEST: Factorial\n============================================================\ndef main():\n    n = int(input())\n    if n == 0 or n == 1:\n        return 1\n    else:\n        return n * factorial(n-1)\n    return n * factorial(n-1) + n * factorial(n-1) - n * factorial(n-1) / n\n\n============================================================\nTEST: Find Maximum\n============================================================\ndef max_max_in_list():\n    max = max(0, max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max(max\n\n============================================================\nTEST: Palindrome Check\n============================================================\ndef main():\n    s = 0\n    for n in range(0, len(s)):\n        n, m = map(int, s.split())\n        if s[s.find(',') + 1] == ';':\n            return True\n        else:\n            return False\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import shutil\nmodel_dir = \"/kaggle/working/codeparrot-lora-improved\"\nzip_path = \"/kaggle/working/codeparrot-lora-improved\"\nshutil.make_archive(zip_path, 'zip', model_dir)\nprint(f\"CodeParrot model zipped: {zip_path}.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:32:07.874596Z","iopub.execute_input":"2025-11-03T11:32:07.875273Z","iopub.status.idle":"2025-11-03T11:32:09.392156Z","shell.execute_reply.started":"2025-11-03T11:32:07.875253Z","shell.execute_reply":"2025-11-03T11:32:09.391495Z"}},"outputs":[{"name":"stdout","text":"CodeParrot model zipped: /kaggle/working/codeparrot-lora-improved.zip\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Combing Base + LoRA for Gradio","metadata":{}},{"cell_type":"code","source":"# merge_gpt2.py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport os, shutil\n\nbase_name = \"gpt2\"\nlora_dir = \"/kaggle/working/gpt2-lora-improved\"   \nmerged_dir = \"/kaggle/working/gpt2-merged\"\n\nif os.path.isdir(lora_dir) and os.path.exists(os.path.join(lora_dir,\"tokenizer.json\")):\n    tokenizer = AutoTokenizer.from_pretrained(lora_dir)\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(base_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n\nbase = AutoModelForCausalLM.from_pretrained(base_name, torch_dtype=torch.float16, device_map=\"auto\")\n# load LoRA adapter on top of base\npeft_model = PeftModel.from_pretrained(base, lora_dir)\n\nmerged = peft_model.merge_and_unload()   \n# save merged model + tokenizer\nmerged.save_pretrained(merged_dir)\ntokenizer.save_pretrained(merged_dir)\n\nshutil.make_archive(merged_dir, 'zip', merged_dir)\nprint(\"Saved and zipped:\", merged_dir + \".zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:32:18.454214Z","iopub.execute_input":"2025-11-03T11:32:18.454850Z","iopub.status.idle":"2025-11-03T11:32:31.081217Z","shell.execute_reply.started":"2025-11-03T11:32:18.454784Z","shell.execute_reply":"2025-11-03T11:32:31.080502Z"}},"outputs":[{"name":"stdout","text":"Saved and zipped: /kaggle/working/gpt2-merged.zip\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# merge_codeparrot.py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport os, shutil\n\nbase_name = \"codeparrot/codeparrot-small\"\nlora_dir = \"/kaggle/working/codeparrot-lora-improved\"\nmerged_dir = \"/kaggle/working/codeparrot-merged\"\n\nif os.path.isdir(lora_dir) and os.path.exists(os.path.join(lora_dir,\"tokenizer.json\")):\n    tokenizer = AutoTokenizer.from_pretrained(lora_dir)\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(base_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nbase = AutoModelForCausalLM.from_pretrained(base_name, torch_dtype=torch.float16, device_map=\"auto\")\npeft_model = PeftModel.from_pretrained(base, lora_dir)\nmerged = peft_model.merge_and_unload()\nmerged.save_pretrained(merged_dir)\ntokenizer.save_pretrained(merged_dir)\n\nshutil.make_archive(merged_dir, 'zip', merged_dir)\nprint(\"Saved and zipped:\", merged_dir + \".zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T11:32:31.082257Z","iopub.execute_input":"2025-11-03T11:32:31.082499Z","iopub.status.idle":"2025-11-03T11:32:42.731620Z","shell.execute_reply.started":"2025-11-03T11:32:31.082479Z","shell.execute_reply":"2025-11-03T11:32:42.730820Z"}},"outputs":[{"name":"stdout","text":"Saved and zipped: /kaggle/working/codeparrot-merged.zip\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}