{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers datasets peft accelerate evaluate tokenizers\n\nimport os, random, torch, pandas as pd, numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig, get_peft_model\nimport evaluate\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nrandom.seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:17:16.791836Z","iopub.execute_input":"2025-11-12T18:17:16.792077Z","iopub.status.idle":"2025-11-12T18:19:20.203976Z","shell.execute_reply.started":"2025-11-12T18:17:16.792060Z","shell.execute_reply":"2025-11-12T18:19:20.203318Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.4.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.16.0)\nCollecting peft\n  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\nCollecting accelerate\n  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\nCollecting tokenizers\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.18)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.1.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, accelerate, peft, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.9.0\n    Uninstalling accelerate-1.9.0:\n      Successfully uninstalled accelerate-1.9.0\n  Attempting uninstall: peft\n    Found existing installation: peft 0.16.0\n    Uninstalling peft-0.16.0:\n      Successfully uninstalled peft-0.16.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.11.0 evaluate-0.4.6 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.17.1 pyarrow-22.0.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"},{"name":"stderr","text":"2025-11-12 18:18:57.445860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762971537.663595      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762971537.728255      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:26:02.410463Z","iopub.execute_input":"2025-11-12T18:26:02.411196Z","iopub.status.idle":"2025-11-12T18:26:16.144677Z","shell.execute_reply.started":"2025-11-12T18:26:02.411167Z","shell.execute_reply":"2025-11-12T18:26:16.143842Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                              id  \\\n0       0001d1afc246a7964130f43ae940af6bc6c57f01   \n1       0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n2       00027e965c8264c35cc1bc55556db388da82b07f   \n3       0002c17436637c4fe1837c935c04de47adb18e9a   \n4       0003ad6ef0c37534f80b55b4235108024b407f0b   \n...                                          ...   \n287108  fffdfb56fdf1a12d364562cc2b9b1d4de7481dee   \n287109  fffeecb8690b85de8c3faed80adbc7a978f9ae2a   \n287110  ffff5231e4c71544bc6c97015cdb16c60e42b3f4   \n287111  ffff924b14a8d82058b6c1c5368ff1113c1632af   \n287112  ffffd563a96104f5cf4493cfa701a65f31b06abf   \n\n                                                  article  \\\n0       By . Associated Press . PUBLISHED: . 14:11 EST...   \n1       (CNN) -- Ralph Mata was an internal affairs li...   \n2       A drunk driver who killed a young woman in a h...   \n3       (CNN) -- With a breezy sweep of his pen Presid...   \n4       Fleetwood are the only team still to have a 10...   \n...                                                   ...   \n287108  By . James Rush . Former first daughter Chelse...   \n287109  An apologetic Vanilla Ice has given his first ...   \n287110  America's most lethal sniper claimed he wished...   \n287111  By . Sara Malm . PUBLISHED: . 12:19 EST, 8 Mar...   \n287112  (CNN)Former Florida Gov. Jeb Bush has decided ...   \n\n                                               highlights  \n0       Bishop John Folda, of North Dakota, is taking ...  \n1       Criminal complaint: Cop used his role to help ...  \n2       Craig Eccleston-Todd, 27, had drunk at least t...  \n3       Nina dos Santos says Europe must be ready to a...  \n4       Fleetwood top of League One after 2-0 win at S...  \n...                                                   ...  \n287108  Chelsea Clinton said question of running for o...  \n287109  Vanilla Ice, 47 - real name Robert Van Winkle ...  \n287110  America's most lethal sniper made comment in i...  \n287111  A swarm of more than one million has crossed b...  \n287112  Other 2016 hopefuls maintain that Bush's annou...  \n\n[287113 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n      <td>Criminal complaint: Cop used his role to help ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n      <td>A drunk driver who killed a young woman in a h...</td>\n      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n      <td>Nina dos Santos says Europe must be ready to a...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n      <td>Fleetwood are the only team still to have a 10...</td>\n      <td>Fleetwood top of League One after 2-0 win at S...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>287108</th>\n      <td>fffdfb56fdf1a12d364562cc2b9b1d4de7481dee</td>\n      <td>By . James Rush . Former first daughter Chelse...</td>\n      <td>Chelsea Clinton said question of running for o...</td>\n    </tr>\n    <tr>\n      <th>287109</th>\n      <td>fffeecb8690b85de8c3faed80adbc7a978f9ae2a</td>\n      <td>An apologetic Vanilla Ice has given his first ...</td>\n      <td>Vanilla Ice, 47 - real name Robert Van Winkle ...</td>\n    </tr>\n    <tr>\n      <th>287110</th>\n      <td>ffff5231e4c71544bc6c97015cdb16c60e42b3f4</td>\n      <td>America's most lethal sniper claimed he wished...</td>\n      <td>America's most lethal sniper made comment in i...</td>\n    </tr>\n    <tr>\n      <th>287111</th>\n      <td>ffff924b14a8d82058b6c1c5368ff1113c1632af</td>\n      <td>By . Sara Malm . PUBLISHED: . 12:19 EST, 8 Mar...</td>\n      <td>A swarm of more than one million has crossed b...</td>\n    </tr>\n    <tr>\n      <th>287112</th>\n      <td>ffffd563a96104f5cf4493cfa701a65f31b06abf</td>\n      <td>(CNN)Former Florida Gov. Jeb Bush has decided ...</td>\n      <td>Other 2016 hopefuls maintain that Bush's annou...</td>\n    </tr>\n  </tbody>\n</table>\n<p>287113 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Install required packages\n!pip install -q -U transformers datasets peft accelerate tokenizers\n\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\ntrain_csv = \"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\"\noutput_dir = \"/kaggle/working/t5-lora-summarization\"\nmodel_name = \"t5-small\"\n\n\nsample_fraction = 0.025\n\n# Hyperparameters\nmax_input_length = 512\nmax_target_length = 128\ntrain_batch_size = 8\ngradient_accumulation_steps = 2\nnum_train_epochs = 3\nlearning_rate = 3e-4\nfp16 = torch.cuda.is_available()\n\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"Loading dataset...\")\ndf = pd.read_csv(train_csv)\n\n# Sample 10% of data\ndf_sample = df.sample(frac=sample_fraction, random_state=42).reset_index(drop=True)\nprint(f\"Using {len(df_sample)} samples ({sample_fraction*100}% of data)\")\n\n# I am creating my dataset\nds_train = Dataset.from_pandas(df_sample[[\"article\", \"highlights\"]])\n\nprint(\"Loading model and tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=4,\n    lora_alpha=8,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n\nprefix = \"summarize: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + str(x) for x in examples[\"article\"]]\n    targets = [str(x) for x in examples[\"highlights\"]]\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=max_input_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    labels = tokenizer(\n        text_target=targets, \n        max_length=max_target_length, \n        truncation=True, \n        padding=\"max_length\"\n    )\n    \n    # Replace padding token id with -100 for loss calculation\n    labels[\"input_ids\"] = [\n        [(l if l != tokenizer.pad_token_id else -100) for l in label] \n        for label in labels[\"input_ids\"]\n    ]\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"Tokenizing dataset...\")\ntokenized_train = ds_train.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=ds_train.column_names\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer, \n    model=model, \n    label_pad_token_id=-100\n)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    learning_rate=learning_rate,\n    num_train_epochs=1,\n    logging_steps=350,\n    save_steps=500,\n    save_total_limit=2,\n    fp16=fp16,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"Starting training...\")\ntrainer.train()\n\nprint(\"Saving model...\")\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")\n\ndef summarize_text(text, max_length=128, num_beams=4):\n    input_text = prefix + text\n    inputs = tokenizer(\n        input_text, \n        return_tensors=\"pt\", \n        truncation=True, \n        max_length=max_input_length\n    ).to(model.device)\n    \n    outputs = model.generate(\n        **inputs,\n        max_length=max_length, \n        num_beams=num_beams, \n        early_stopping=True\n    )\n    \n    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return summary\n\n# Quick test\nprint(\"\\n\" + \"=\"*80)\nprint(\"Testing on sample articles:\")\nprint(\"=\"*80)\nfor i in range(3):\n    item = ds_train[i]\n    print(f\"\\nARTICLE {i+1} (first 300 chars):\")\n    print(item[\"article\"][:300] + \"...\")\n    print(f\"\\nREFERENCE: {item['highlights']}\")\n    print(f\"\\nPREDICTION: {summarize_text(item['article'])}\")\n    print(\"-\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T18:43:56.519754Z","iopub.execute_input":"2025-11-12T18:43:56.520451Z","iopub.status.idle":"2025-11-12T18:47:42.221749Z","shell.execute_reply.started":"2025-11-12T18:43:56.520429Z","shell.execute_reply":"2025-11-12T18:47:42.221000Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Loading dataset...\nUsing 7178 samples (2.5% of data)\nLoading model and tokenizer...\ntrainable params: 147,456 || all params: 60,654,080 || trainable%: 0.2431\nTokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f2d96d8e9e4d11b5d12d5fd21a51d2"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/3984938252.py:138: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [225/225 03:02, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving model...\nModel saved to /kaggle/working/t5-lora-summarization\n\n================================================================================\nTesting on sample articles:\n================================================================================\n\nARTICLE 1 (first 300 chars):\nBy . Mia De Graaf . Britons flocked to beaches across the southern coast yesterday as millions look set to bask in glorious sunshine today. Temperatures soared to 17C in Brighton and Dorset, with people starting their long weekend in deck chairs by the sea. Figures from Asda suggest the unexpected s...\n\nREFERENCE: People enjoyed temperatures of 17C at Brighton beach in West Sussex and Weymouth in Dorset .\nAsda claims it will sell a million sausages over long weekend despite night temperatures dropping to minus 1C .\nBut the good weather has not been enjoyed by all as the north west and Scotland have seen heavy rain .\n\nPREDICTION: Temperatures soared to 17C in Brighton and Dorset. Forecasters predict dry weather across southern England, southern Wales and the south Midlands.\n--------------------------------------------------------------------------------\n\nARTICLE 2 (first 300 chars):\nA couple who weighed a combined 32st were shamed into slimming by their own family - during Christmas dinner. Margaret Gibson, 37, and her husband, James, 41, from Biddulph, Staffs, started piling on the pounds after the birth of their two children just over a decade ago. But after taunts during the...\n\nREFERENCE: Couple started piling on pounds after the birth of two children .\nMargaret Gibson weighed 12st 5lb and husband James weighed 20st .\nJames Gibson's barred from simple op as he 'would die', warned doctor .\n\nPREDICTION: Margaret Gibson, 37, and her husband, James, 41, started piling on the pounds after their two children just over a decade ago. After taunts from James' doctor that he couldn't undergo a procedure because he would 'die on the operating table' the pair took action and have lost 7st between them since joining Weight Watchers in January.\n--------------------------------------------------------------------------------\n\nARTICLE 3 (first 300 chars):\nVideo footage shows the heart stopping moment a 17 year old boy was bitten on the hand by a shark. Sam Smith's Go-Pro vision helmet captured the sudden and brief attack while he was spear fishing on the New South Wales south coast on Friday. The teenager can be seen trying to stab the shark with his...\n\nREFERENCE: A 17-year-old boy suffering lacerations to his left hand and fingers .\nSam Smith was spearfishing when he saw a shark 'so went down to film it'\nGo-Pro vision helmet captured attack on the NSW south coast on Friday .\nHe tried to stab it with his spear but the shark 'went ballistic and bit him'\nSam was airlifted from Mollymook beach to Sydney to undergo surgery .\nMeanwhile, Manly beach closed on Sunday due to a shark sighting .\nNewcastle beach remains closed for ninth consecutive day from a sighting .\n\nPREDICTION: Sam Smith's Go-Pro Vision helmet captured the sudden and brief attack while he was spear fishing on the New South Wales south coast on Friday. Sam tried to stab the predator with his spear gun as the 1.5metre shark aggressively swam away. He was airlifted from Mollymook Beach to Sydney to undergo surgery.\n--------------------------------------------------------------------------------\n\nZipping model...\nModel zipped: /kaggle/working/t5-lora-summarization.zip\n\nDone! ✓\n","output_type":"stream"}],"execution_count":6}]}